{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicios Clase 2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTM1vj54iegu"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Ejercicio #1-C2  Datasets sintético**\n",
        "\n",
        "En problemas de Machine Learning es muy importante contar con el dataset correcto. Pero muchas veces, el dataset no es fácil conseguir o el equipo de data engineers aún lo está generando.\n",
        "\n",
        "Una manera sencilla de generar datos para probar soluciones de Machine Learning es crear datasets sintéticos. Por ejemplo, es simple crear datasets con grados de clusterización variables y probar cómo se comportan nuestros algoritmos en diferentes escenarios. \n",
        "\n",
        "Objetivo: utilizar numpy para crear datos clusterizados A/B en 4 dimensiones.\n",
        "\n",
        "Hint:\n",
        "\n",
        "\n",
        "\n",
        "*   Definir una matriz de centroides [1,0,0,0] y [0,1,0,0]\n",
        "*   Utilizar una constante para separar o alejar los centroides entre si.\n",
        "*   Utilizar np.repeat para crear n/2 muestras de cada centroide.\n",
        "*   Sumar a cada centroide un vector aleatorio normal i.i.d. con media 0 y desvío (np.random.normal)\n",
        "\n",
        "*   Armar un arreglo que tenga n enteros indicando si la muestra pertenece a A o a B\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M0RdLA4Di38V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class DatasetSintetico(object):\n",
        "\n",
        "  def matrizDeCentroides(self,constante):\n",
        "    arr1=np.array([1,0,0,0])\n",
        "    arr2=np.array([0,1,0,0])\n",
        "    matriz=np.matrix((arr1,arr2))\n",
        "    matriz=np.multiply(matriz,constante)\n",
        "    return matriz\n",
        "\n",
        "  def datos(self, numDatos):\n",
        "    cent=self.matrizDeCentroides(constante)\n",
        "    data=np.repeat(cent,numDatos/2,axis=0)\n",
        "    return data\n",
        "\n",
        "  def desvest_generador(self,numDatos):\n",
        "    media=0\n",
        "    desvest=np.random.normal(loc=media,scale=1,size=(numDatos,4))\n",
        "    return desvest\n",
        "\n",
        "  def respFinal(self,numDatos):\n",
        "    #Lo corro una vez por cluster:\n",
        "    datosA=self.datos(numDatos)\n",
        "    desvestA=self.desvest_generador(numDatos)\n",
        "    vectorAleatorioA=datosA+desvestA\n",
        "    #0 Representa el Cluster A\n",
        "    vectorAleatorioA[5,:]=0\n",
        "    datosB=self.datos(numDatos)\n",
        "    desvestB=self.desvest_generador(numDatos)\n",
        "    vectorAleatorioB=datosB+desvestB\n",
        "    #1 Representa el Cluster B\n",
        "    vectorAleatorioA[5,:]=1\n",
        "    respuestaFinal=np.vstack((vectorAleatorioA,vectorAleatorioB))\n",
        "    return respuestaFinal\n",
        "#Creo una nueva instancia\n",
        "dt=DatasetSintetico()\n",
        "\n",
        "#Creo la constante que va a separar o acercar los centroides\n",
        "constante=2\n",
        "\n",
        "#Indico cuántos datos necesito\n",
        "datos=1000\n",
        "\n",
        "print(dt.respFinal(datos))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q09AgcTAGzN5",
        "outputId": "47d71f9b-86c9-469e-f3b9-cb0d34692678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 3.16157209 -0.96057197  0.37335445  0.71020885]\n",
            " [ 0.47884511 -0.70912936  0.70352921 -0.16304079]\n",
            " [ 2.66970981 -0.74228334  1.11645347  1.92848938]\n",
            " ...\n",
            " [-0.80269634  0.22250967 -0.71559753  0.97591323]\n",
            " [-0.33155228  2.76831632  0.88315121 -0.24547869]\n",
            " [ 0.10539838  1.02185298  0.69370816  0.77123347]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Ejercicio #1 Dado un dataset X, calcular PCA para reducir dimensión**\n",
        "\n",
        "Siguiendo los pasos vistos en la teoría, se requiere utilizar numpy para calcular PCA del dataset de entrada X, utilizando los componentes más importantes.\n",
        "\n",
        "                     x= np.array([[0.8, 0.7] , [0.1, -0,1]])\n",
        "\n",
        " Al finalizar la implementación en numpy, corroborar obtener los mismos resultados que utilizando el código de la librería scikit-learn. Escribir un test para comprobar las matrices.                 "
      ],
      "metadata": {
        "id": "wNJDf7c5ouQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importamos las librerías necesarias para poder hacer la comparación con Sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "56LZLvpBSYoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dado un dataset con n muestras y d features, queremos reducir sus diemnsiones a m. Para ello, el primer paso es centrar el dataset.\n",
        "2. Obtener la matriz de covarianza X Transpuesta.\n",
        "3. Calcular los autovalores y autovectores de la matriz de covarianza. Revisar la documentación de Numpy.\n",
        "4. Ordenar los autovalores y autovectores de la matriz de covarianza.\n",
        "5. Ordenar los autovectores en el sentido de los autovalores decrecientes.\n",
        "6. Proyectar el dataset centrado sobre los m autocetores más relevantes usando np.dot.\n",
        "7. Consolidar los pasos anteriores en una función"
      ],
      "metadata": {
        "id": "05YgiNlJS55a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definimos el dataset\n",
        "x=np.array([[0.8,0.7],[0.1,-0.1]])\n",
        "print(x)\n",
        "\n",
        "def pca(x):\n",
        "  #Centramos el dataset\n",
        "  xMedia=np.mean(x,axis=0)\n",
        "  xCentrado=x-xMedia\n",
        "\n",
        "  #Trasponemos la matriz xCentrada\n",
        "  transpuesta=np.matrix.transpose(xCentrado)\n",
        "  covarianza=np.cov(transpuesta)\n",
        "\n",
        "  #Calculamos los Eigenvalues y Eigenvectors\n",
        "  eigenValores, eigenVectores = np.linalg.eig(covarianza)\n",
        "\n",
        "  #Organizamos los autovalores y autovectores.\n",
        "  #Sort regresa la lista organizada, mientras que argsort regresa los índices.\n",
        "  eigenValoresS=np.argsort(eigenValores)\n",
        "  eigenVectoresS=np.argsort(eigenVectores)\n",
        "\n",
        "  #Ahora los organizamos de mayor a menor\n",
        "  eigenValoresS2=eigenValoresS[::-1]\n",
        "  eigenVectoresS2=eigenVectoresS[::-1]\n",
        "\n",
        "  #Finalmente, proyectamos usando np.dot\n",
        "  proy=np.dot(xCentrado,eigenVectoresS2)\n",
        "  print(proy)\n",
        "\n",
        "k=pca(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNycLPhWQDmg",
        "outputId": "b12321a8-39ca-4370-892b-af80d025a65e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.8  0.7]\n",
            " [ 0.1 -0.1]]\n",
            "[[ 0.8  0.7]\n",
            " [ 0.1 -0.1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Ejercicio #2  Aplicar PCA y K-means al datasets de dígitos**\n",
        "\n",
        "Siguiendo los ejemplos vistos en clase sobre los datasets del Human Activity Recognition y Fashion MNIST, realizar las siguientes consignas:\n",
        "\n",
        "\n",
        "\n",
        "1.   Aplicar PCA sobre el dataset para poder explicar el 90% de la varianza. ¿cuántos componentes se requieren?\n",
        "2.  Graficar un screen plot ( varianza contemplada en función del número de componentes considerados)\n",
        "3. Visualizar gráficamente los primeros 5 componentes ¿Qué conclusiones se puede sacar de cada componente? [OPCIONAL]\n",
        "4. Visualizar la imágen original vs la reconstruida con los m componentes del punto 1.\n",
        "5. Graficar una matriz de correlación del dataset reducido.\n",
        "6. Graficar los clusters de dígitos en 2 y 3 dimensiones usando los componentes obtenidos en PCA.\n",
        "7. Aplicar K-means para clusterizar los dígitos ¿cómo son los resultados?\n",
        "8.Realizar un gráfico de inercia para obtener el número óptimo de clusters *K.*\n",
        "9. Analizar visualmente los límites de cluster de algún dígito y \"generar\" artificialmente el dígito dándole valores a los primeros dos componentes de PCA.\n"
      ],
      "metadata": {
        "id": "oEm4bEH0l0Z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Según lo dado en el ejemplo Fashion MNIST\n",
        "# Cargamos los datos de training\n",
        "X, y = load_digits(return_X_y=True)\n",
        "print('Formato del dataset: {}'.format(X.shape))"
      ],
      "metadata": {
        "id": "J3jXFDrRNyzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Ejercicio integrador**\n",
        "\n",
        "1. Generar un dataset sintético con una función seno (pueden usar otras funciones no lineales)\n",
        "2. Hacer el gráfico de los datos\n",
        "3. Hacer fit con la solución cerrada de regresión lineal y con los algoritmos de descenso por el gradiente vistos en clase, para diferentes polinomios hasta el orden 15. Comenten los problemas numéricos encontrados.\n",
        "4. Obtener mediante cross-validation para cada polinomio el error de validación.\n",
        "5. Seleccionar el modelo con complejidad correcta para el dataset.\n",
        "6. Regularizar el modelo y comparar los resultados."
      ],
      "metadata": {
        "id": "DKNasW-4zG2j"
      }
    }
  ]
}